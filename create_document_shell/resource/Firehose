#!/bin/bash
# -----------------------------------------------------------------------------
# Kinesis Firehose情報取得リソースファイル(Mac/Linux用)
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# 【環境変数】
# OS_TYPE         ：OS種別
# OVERWRITE_OPTION：sedコマンドのオプション
# ITEMS           ：セクション数のカウント初期化処理
# ROW_COUNT       ：項目行の行数カウント初期化処理
# AWS_REGION      ：情報を取得するリージョンの指定【編集可】
#                   複数リージョンを取得する場合はカンマ（,）で区切る
# LIST_TMP        ：リージョンリスト作成用tmpファイル
# FIREHOSE_TMP    ：Kinesis Firehoseリスト作成用tmpファイル
# -----------------------------------------------------------------------------
OS_TYPE=$(uname -s)
if [ ${OS_TYPE} == "Darwin" ]; then
  OVERWRITE_OPTION=".tmp"
else
  OVERWRITE_OPTION=""
fi

ITEMS="0"
ROW_COUNT[${ITEMS}]="0"
AWS_REGION="ap-northeast-1,us-east-1"
LIST_TMP=.${RESOURCE_NAME}.tmp
echo ${AWS_REGION} | sed -e "s/ //g" -e "s/,/\n/g" >| ${LIST_TMP}
FIREHOSE_TMP=".firehose_list.tmp"

# -----------------------------------------------------------------------------
# 【メイン処理】
# -----------------------------------------------------------------------------
# タイトル【編集可】
echo "# Kinesis Firehose" >| ${OUTPUT_PATH}

# セクション数変数のインクリメント
ITEMS=$((${ITEMS}+1))

while read REGION_LIST
do
  # LoadBalancerArnのリスト作成【編集可】
  aws firehose list-delivery-streams --limit 100 --region ${REGION_LIST} | jq -r '.DeliveryStreamNames[]' | sort >| ${FIREHOSE_TMP}

  # リージョンごとの表題【編集可】
  echo "### 配信ストリーム (${REGION_LIST})" >> ${OUTPUT_PATH}
  echo "" >> ${OUTPUT_PATH}

  # セクション数変数のインクリメント
  ITEMS=$((${ITEMS}+1))

  # 項目行の行数カウント処理
  ROW_COUNT[${ITEMS}]=$(wc -l ${OUTPUT_PATH} | awk '{print $1}')

  while read FIREHOSE_LIST
  do
    # リソースリスト取得【編集可】
    aws firehose describe-delivery-stream --delivery-stream-name ${FIREHOSE_LIST} --region ${REGION_LIST} | jq -r '.DeliveryStreamDescription |
    [
      .DeliveryStreamName, # 配信ストリーム名
      .DeliveryStreamType, # ソース
      (.Destinations[].ExtendedS3DestinationDescription.ProcessingConfiguration.Processors[].Type // "有効ではありません"), # 変換タイプ
      ((.Destinations[].ExtendedS3DestinationDescription.ProcessingConfiguration.Processors[].Parameters[] | select(.ParameterName == "LambdaArn").ParameterValue | split(":") | .[-2]) // "有効ではありません"), # Lambda関数
      (.Destinations[].S3DestinationDescription.CloudWatchLoggingOptions.LogGroupName), # ロググループ名
      (.Destinations[].S3DestinationDescription.CloudWatchLoggingOptions.LogStreamName) # ログストリーム名
    ] | @tsv' | sort | sed -e "s/\t/|/g" -e "s/^/|/g" -e "s/$/|/g" -e "s/\"//g" >> ${OUTPUT_PATH}

  done < ${FIREHOSE_TMP}

    # 項目行とMarkdown形式の表の書式行追加【編集可】
    sed -i ${OVERWRITE_OPTION} -e "$((${ROW_COUNT[${ITEMS}]}+1))s/^/|配信ストリーム名|ソース|変換タイプ|データ変換|ロググループ名|ログストリーム名|\n/" ${OUTPUT_PATH}
    sed -i ${OVERWRITE_OPTION} -e "$((${ROW_COUNT[${ITEMS}]}+2))s/^/|:--|:--|:--|:--|:--|:--|\n/" ${OUTPUT_PATH}

    # 項目行の行数カウント処理
    ROW_COUNT[${ITEMS}]=$(wc -l ${OUTPUT_PATH} | awk '{print $1}')

    # 改行【編集可】
    echo "" >> ${OUTPUT_PATH}

  # CSVファイルへの変換処理
  if [ ${FILETYPE} == "csv" ]; then
    EXPORT_CSV ${ITEMS} ${OUTPUT_PATH}
  fi

done < ${LIST_TMP}

# 作業用tmpファイルの削除
rm -f ${LIST_TMP}
rm -f ${FIREHOSE_TMP}

if [ ${OS_TYPE} == Darwin ]; then
  rm -f ${OUTPUT_PATH}${OVERWRITE_OPTION}
fi